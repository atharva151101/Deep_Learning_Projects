[[8.5300e+02 1.3740e+03 1.9110e+03 2.7510e+03 4.0080e+03 5.4470e+03
  6.5740e+03 6.8930e+03 6.5290e+03 6.0560e+03 5.4650e+03 4.4570e+03
  2.9520e+03 1.9370e+03 1.3730e+03 4.4200e+03 1.0000e+00 5.2000e+01
  1.1800e+02 2.5800e+02 8.2900e+02 1.2160e+03 1.4420e+03 3.0290e+03
  5.0310e+03 8.5880e+03 1.0688e+04 1.0144e+04 8.3640e+03 5.3440e+03
  2.4240e+03 5.4720e+03 4.1000e+01 1.3000e+02 5.3300e+02 1.1680e+03
  1.5680e+03 2.7730e+03 4.7260e+03 8.1810e+03 9.3580e+03 8.5310e+03
  7.3660e+03 5.7770e+03 3.8800e+03 2.5640e+03 1.5360e+03 4.8680e+03]] [ 0.1946464   0.2476869   0.19582312 -0.16379535 -0.3641333  -0.09878844
  0.11702633 -0.14105457 -0.00100929  0.34745818  0.01708617 -0.4448244
  0.27992278 -0.17124115  0.09294281  0.03567962  0.09562296 -0.15869904
  0.2400177   0.06388915  0.1974225   0.35252652  0.3424704   0.16085234
  0.24697046 -0.39717487 -0.28725857 -0.20361048  0.01850308 -0.22289073
 -0.18778087 -0.0578709   0.65745676  0.49976832 -0.32705072  0.0064458
 -0.4785666  -0.03902976 -0.14906837  0.6278702  -0.14817393  0.0647389
  0.43524474 -0.03236784  0.333839   -0.401352    0.28355217  0.08547091]
/home/atharva1511/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1, 48])) that is different to the input size (torch.Size([48])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Epoch - 0	 step - 12669 	Train loss - 25289931.855290547 	 MAE - 137749.38659465642
Val loss: 22739634.588119082	 Val error: 135651.23414669864
[[1431. 2265. 3050. 4474. 6061. 7696. 9226. 9317. 7745. 5161. 2842. 1609.
   876.  647.  420.  180.  524. 1989. 2301. 3703. 5291. 6984. 8534. 9423.
  8726. 6522. 3914. 2238. 1287.  753.  509.  302.  440. 1764. 1971. 3109.
  4593. 6227. 7827. 9700. 9396. 7179. 4562. 2710. 1627.  909.  613.  373.]] [5527.746  4873.851  5376.9434 5703.9097 5728.9326 5383.178  4929.543
 4545.4277 3979.31   3540.2744 3108.1384 2651.5994 2352.4424 1870.8401
 1513.8468 3444.8345 2870.561  2661.2378 3129.6123 3590.4944 4090.0159
 4624.676  5102.554  5550.994  5673.585  5569.984  5192.026  4549.3867
 3744.7659 2765.5715 2012.1239 3403.511  2669.606  2373.3047 2833.0813
 3459.1475 4128.2124 4576.518  4886.0127 5302.08   5244.694  5116.8345
 4922.0786 4592.4614 4205.5933 3483.1628 2674.0767 4063.9946]
Epoch - 1	 step - 12669 	Train loss - 22042084.90618094 	 MAE - 134290.46836415376
Val loss: 21496400.88125	 Val error: 133358.79729106033
[[ 5608.  2602.  2888.  5614.  6186.  3955.  3739.  4342.  8498.  2925.
   1750.  1413.  1491.  6162.   896.  4931.  5235.  2550.  1919.  1680.
   1509.  2064.  2778.  4927.  9533. 11461.  3546.  1605.  1612.  6480.
   1057.  5044.  5075.  2545.  1846.  1710.  1594.  2457.  5531.  7923.
   5832.  8273.  4153.  1718.  1701.  6440.   962.  5240.]] [5018.2744 4514.9717 5170.3467 5649.621  5728.3833 5535.5674 5117.4673
 4700.3267 4115.8345 3675.085  3159.3157 2849.6475 2487.318  1884.4224
 1438.4048 3301.1035 2585.2908 2398.1392 2967.7224 3527.2676 4102.8013
 4617.8555 5185.9    5658.397  5858.557  5723.731  5311.676  4591.688
 3873.1987 2762.832  1968.2881 3212.913  2368.5532 2113.6821 2629.8242
 3364.0117 4084.7358 4561.063  4961.633  5370.3022 5419.579  5351.9883
 5082.124  4800.803  4272.0513 3356.0698 2579.9153 4029.7815]
Epoch - 2	 step - 12669 	Train loss - 21166398.58826707 	 MAE - 132445.65465975236
Val loss: 20888060.808334157	 Val error: 131826.97441020867
[[4.2581e+04 7.2680e+03 4.8000e+03 3.4850e+03 2.3550e+03 1.0550e+03
  4.5300e+02 3.9900e+02 3.2100e+02 1.3400e+02 8.5000e+01 4.5000e+01
  1.9000e+01 0.0000e+00 0.0000e+00 0.0000e+00 6.9820e+03 6.4450e+03
  5.1600e+02 5.3800e+02 7.2500e+02 8.1300e+02 1.1310e+03 1.4470e+03
  1.7620e+03 4.6750e+03 1.7691e+04 1.5625e+04 4.4200e+03 2.0900e+02
  2.1000e+01 0.0000e+00 7.7270e+03 5.5940e+03 3.8400e+02 3.5100e+02
  3.3800e+02 4.8600e+02 7.0600e+02 4.1510e+03 1.7617e+04 1.3695e+04
  6.2890e+03 2.0990e+03 1.3810e+03 1.1970e+03 5.5000e+02 4.3500e+02]] [11049.94     7853.2646   5796.658    4384.5557   3561.6426   2860.95
  2434.3047   2138.1133   1913.1636   1830.207    2044.3867   1598.0543
  1546.6321   1764.138    1390.571   -1913.1302   3836.965    5035.4194
  4643.803    4427.933    4093.6592   3603.9192   3300.2283   3210.252
  3076.3384   3194.6978   3323.2336   3178.5002   3204.6313   2264.0205
  1796.7949  -1939.28     3845.039    5160.011    5075.396    4364.5674
  3587.3914   3165.579    2838.0261   2874.8823   3141.2107   3377.9924
  3542.0256   2927.958    2454.853    2136.563    2574.299    -815.21387]
Epoch - 3	 step - 12669 	Train loss - 20706897.589275848 	 MAE - 131273.66549705257
Val loss: 20541848.28324043	 Val error: 130733.35192198599
[[1097. 3613. 5729. 7321. 9478. 9520. 7044. 5325. 3735. 2304. 1367.  778.
   551.  442.  526. 4170.  220. 2065. 3524. 5078. 5312. 5626. 6208. 6275.
  5453. 5494. 6463. 4305. 1556.  615.  554. 4252.  254. 1898. 3054. 4387.
  5294. 6224. 6766. 8464. 8114. 5005. 3328. 2432. 1496. 1178.  904. 4202.]] [4708.0967 4452.914  5214.532  5731.74   5892.2954 5717.297  5273.538
 4820.95   4184.0103 3648.1514 3177.301  2771.6448 2410.3364 1759.9944
 1404.355  2602.2173 2530.5024 2339.3635 2934.295  3482.9917 4113.2544
 4726.233  5360.144  5887.4507 6085.3486 5787.8643 5326.449  4479.8506
 3692.9614 2674.2969 1846.009  2502.3901 2325.9556 1978.8306 2498.884
 3282.5632 4091.0923 4696.1626 5197.7905 5591.4507 5566.859  5430.476
 5097.433  4734.372  4257.5513 3350.0583 2434.9248 3234.983 ]
Epoch - 4	 step - 12669 	Train loss - 20426764.084244277 	 MAE - 130377.9828897309
Val loss: 20317797.449329123	 Val error: 130438.78620557296
[[5638. 6928. 7061. 5344. 4265. 4268. 3856. 3206. 2886. 2596. 3492. 4424.
  1934.  813. 1352. 4937. 4130. 6435. 6047. 7052. 5703. 4431. 3076. 2056.
  1925. 2407. 2429. 4233. 4965. 1298. 1134. 5679. 7813. 5707. 3921. 3648.
  3407. 4091. 3836. 2255. 1540. 1833. 2498. 2789. 2896. 3834. 5848. 7084.]] [4658.049  4193.2715 5127.302  5804.0654 5869.737  5726.4565 5315.097
 4827.7344 4240.1143 3723.9575 3145.66   2814.2275 2478.6414 1868.4836
 1465.2446 2388.3665 2570.0254 2138.802  2736.6323 3379.668  4089.879
 4718.4233 5332.497  5871.446  6040.3804 5777.094  5371.2393 4676.8545
 3874.1548 2846.7295 1975.2639 2247.3096 2398.797  1866.3059 2418.037
 3241.0234 4032.2458 4670.842  5157.7715 5547.5166 5452.662  5289.097
 5033.439  4917.909  4406.446  3526.2366 2556.667  3131.3975]
Epoch - 5	 step - 12669 	Train loss - 20237120.418454025 	 MAE - 129911.3265538674
Val loss: 20153330.157660812	 Val error: 129561.5695514441
[[11559.  3086.  3298.  4692.  4249.  4683.  4784.  4449.  3538.  3141.
   2170.  1690.  2168.  2913.  2854.  3726.   307.   579.  1239.  3058.
   3418.  5312.  6263.  6427.  5671.  3754.  3554.  3853.  3116.  4113.
   6411.  5925.  2362.  2430.  3114.  3154.  4769.  4581.  3426.  3907.
   4338.  4807.  3936.  2300.  3384.  4792.  6240.  5460.]] [4605.737  4214.284  5121.518  5812.516  5969.8267 5763.767  5351.8027
 4911.7017 4299.029  3767.494  3180.0513 2808.7583 2364.8953 1693.5278
 1368.5222 2156.6223 2551.6118 2185.262  2809.5881 3417.655  4109.8237
 4765.5996 5382.288  5911.258  6054.4775 5818.674  5465.925  4597.9263
 3773.2507 2674.1162 1822.0796 2050.5378 2321.0933 1869.134  2443.0435
 3202.2466 4068.4521 4775.481  5264.5684 5648.192  5519.7207 5374.4
